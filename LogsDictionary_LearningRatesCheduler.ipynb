{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogsDictionary-LearningRatesCheduler.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+HpDhpXPOayUr+z84pWDb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DommiiUdomp/-/blob/main/LogsDictionary_LearningRatesCheduler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YitHBQsDmFaE"
      },
      "source": [
        "# **Using the logs dictionary** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNkWOSBsmT2g"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the diabetes dataset\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "diabetes_dataset = load_diabetes()\n",
        "\n",
        "# Save the input and target variables\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = diabetes_dataset['data']\n",
        "targets = diabetes_dataset['target']\n",
        "\n",
        "# Split the data set into training and test sets\n",
        "\n",
        "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfMXlHGKmi6w"
      },
      "source": [
        "# Build the model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
        "    Dense(64,activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)        \n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "    \n",
        "model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aid4hKtmmpzV"
      },
      "source": [
        "**Defining a custom callback**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZJOb0Am45o"
      },
      "source": [
        "# Create the custom callback\n",
        "\n",
        "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # Print the loss after every second batch in the training set\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if batch %2 ==0:\n",
        "            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
        "    \n",
        "    # Print the loss after each batch in the test set\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
        "\n",
        "    # Print the loss and mean absolute error after each epoch\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n",
        "    \n",
        "    # Notify the user when prediction has finished on each batch\n",
        "    def on_predict_batch_end(self,batch, logs=None):\n",
        "        print(\"Finished prediction on batch {}!\".format(batch))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub7O78j8m7Gy",
        "outputId": "31d16849-1f31-4155-d09d-6ca66ae4b55e"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "history = model.fit(train_data, train_targets, epochs=20,\n",
        "                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n",
        "                            callbacks=[LossAndMetricCallback()], verbose=False)\n",
        "\n",
        "# Get predictions from the model\n",
        "\n",
        "model_pred = model.predict(test_data, batch_size=10,\n",
        "                           callbacks=[LossAndMetricCallback()], verbose=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " After batch 0, the loss is 28861.44.\n",
            "\n",
            " After batch 2, the loss is 28559.13.\n",
            "Epoch 0: Average loss is 28475.07, mean absolute error is  150.71.\n",
            "\n",
            " After batch 0, the loss is 31238.33.\n",
            "\n",
            " After batch 2, the loss is 29637.67.\n",
            "Epoch 1: Average loss is 28388.03, mean absolute error is  150.45.\n",
            "\n",
            " After batch 0, the loss is 26767.34.\n",
            "\n",
            " After batch 2, the loss is 27961.96.\n",
            "Epoch 2: Average loss is 28275.17, mean absolute error is  150.12.\n",
            "\n",
            " After batch 0, the loss is 30495.00.\n",
            "\n",
            " After batch 2, the loss is 28226.37.\n",
            "Epoch 3: Average loss is 28113.73, mean absolute error is  149.66.\n",
            "\n",
            " After batch 0, the loss is 31848.59.\n",
            "\n",
            " After batch 2, the loss is 27517.75.\n",
            "Epoch 4: Average loss is 27878.24, mean absolute error is  148.99.\n",
            "\n",
            " After batch 0, the loss is 28139.96.\n",
            "\n",
            " After batch 2, the loss is 27176.76.\n",
            "Epoch 5: Average loss is 27526.62, mean absolute error is  148.02.\n",
            "\n",
            " After batch 0, the loss is 27529.09.\n",
            "\n",
            " After batch 2, the loss is 27110.29.\n",
            "Epoch 6: Average loss is 27052.24, mean absolute error is  146.71.\n",
            "\n",
            " After batch 0, the loss is 24758.89.\n",
            "\n",
            " After batch 2, the loss is 26123.45.\n",
            "Epoch 7: Average loss is 26403.56, mean absolute error is  144.87.\n",
            "\n",
            " After batch 0, the loss is 27074.27.\n",
            "\n",
            " After batch 2, the loss is 25119.90.\n",
            "Epoch 8: Average loss is 25541.61, mean absolute error is  142.34.\n",
            "\n",
            " After batch 0, the loss is 24579.90.\n",
            "\n",
            " After batch 2, the loss is 23565.65.\n",
            "Epoch 9: Average loss is 24441.79, mean absolute error is  139.07.\n",
            "\n",
            " After batch 0, the loss is 26684.72.\n",
            "\n",
            " After batch 2, the loss is 24729.75.\n",
            "Epoch 10: Average loss is 23111.93, mean absolute error is  134.82.\n",
            "\n",
            " After batch 0, the loss is 21299.07.\n",
            "\n",
            " After batch 2, the loss is 21860.05.\n",
            "Epoch 11: Average loss is 21494.42, mean absolute error is  129.67.\n",
            "\n",
            " After batch 0, the loss is 20094.67.\n",
            "\n",
            " After batch 2, the loss is 20161.18.\n",
            "Epoch 12: Average loss is 19620.32, mean absolute error is  123.21.\n",
            "\n",
            " After batch 0, the loss is 14665.79.\n",
            "\n",
            " After batch 2, the loss is 15551.85.\n",
            "Epoch 13: Average loss is 17627.10, mean absolute error is  115.85.\n",
            "\n",
            " After batch 0, the loss is 15760.23.\n",
            "\n",
            " After batch 2, the loss is 15642.00.\n",
            "Epoch 14: Average loss is 15401.76, mean absolute error is  106.90.\n",
            "\n",
            " After batch 0, the loss is 12714.12.\n",
            "\n",
            " After batch 2, the loss is 13490.08.\n",
            "Epoch 15: Average loss is 13271.26, mean absolute error is   97.15.\n",
            "\n",
            " After batch 0, the loss is 11943.80.\n",
            "\n",
            " After batch 2, the loss is 11961.67.\n",
            "Epoch 16: Average loss is 11177.19, mean absolute error is   86.93.\n",
            "\n",
            " After batch 0, the loss is 10444.13.\n",
            "\n",
            " After batch 2, the loss is 8936.01.\n",
            "Epoch 17: Average loss is 9249.18, mean absolute error is   76.53.\n",
            "\n",
            " After batch 0, the loss is 8386.72.\n",
            "\n",
            " After batch 2, the loss is 7971.55.\n",
            "Epoch 18: Average loss is 7808.06, mean absolute error is   68.55.\n",
            "\n",
            " After batch 0, the loss is 6932.12.\n",
            "\n",
            " After batch 2, the loss is 6843.52.\n",
            "Epoch 19: Average loss is 6656.99, mean absolute error is   62.86.\n",
            "\n",
            " After batch 0, the loss is 23817.04.\n",
            "\n",
            " After batch 1, the loss is 23450.50.\n",
            "\n",
            " After batch 2, the loss is 22721.01.\n",
            "\n",
            " After batch 3, the loss is 21617.14.\n",
            "\n",
            " After batch 4, the loss is 20508.07.\n",
            "Finished prediction on batch 0!\n",
            "Finished prediction on batch 1!\n",
            "Finished prediction on batch 2!\n",
            "Finished prediction on batch 3!\n",
            "Finished prediction on batch 4!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YviJi8T4nCYl"
      },
      "source": [
        "**Application - learning rate scheduler**\n",
        "\n",
        "Let's now look at a more sophisticated custom callback.\n",
        "We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n",
        "\n",
        "First we define the auxillary function that returns the learning rate for each epoch based on our schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BMmja4pnH1g"
      },
      "source": [
        "# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n",
        "\n",
        "lr_schedule = [\n",
        "    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n",
        "]\n",
        "\n",
        "def get_new_epoch_lr(epoch, lr):\n",
        "    # Checks to see if the input epoch is listed in the learning rate schedule \n",
        "    # and if so, returns index in lr_schedule\n",
        "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n",
        "    if len(epoch_in_sched)>0:\n",
        "        # If it is, return the learning rate corresponding to the epoch\n",
        "        return lr_schedule[epoch_in_sched[0]][1]\n",
        "    else:\n",
        "        # Otherwise, return the existing learning rate\n",
        "        return lr"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_s4GIIqnV7k"
      },
      "source": [
        "# Define the custom callback\n",
        "\n",
        "class LRScheduler(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, new_lr):\n",
        "        super(LRScheduler, self).__init__()\n",
        "        # Add the new learning rate function to our callback\n",
        "        self.new_lr = new_lr\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "              raise ValueError('Error: Optimizer does not have a learning rate.')\n",
        "                \n",
        "        # Get the current learning rate\n",
        "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "        \n",
        "        # Call the auxillary function to get the scheduled learning rate for the current epoch\n",
        "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
        "\n",
        "        # Set the learning rate to the scheduled learning rate\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
        "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDO-hTO8nYVj"
      },
      "source": [
        "# Build the same model as before\n",
        "\n",
        "new_model = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
        "    Dense(64,activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)        \n",
        "])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pQbpqZknaBu",
        "outputId": "5d0790d9-8f27-4375-e330-5168911cc90b"
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "new_model.compile(loss='mse',\n",
        "                optimizer=\"adam\",\n",
        "                metrics=['mae', 'mse'])\n",
        "\n",
        "# Fit the model with our learning rate scheduler callback\n",
        "\n",
        "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
        "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate for epoch 0 is   0.001\n",
            "Learning rate for epoch 1 is   0.001\n",
            "Learning rate for epoch 2 is   0.001\n",
            "Learning rate for epoch 3 is   0.001\n",
            "Learning rate for epoch 4 is   0.030\n",
            "Learning rate for epoch 5 is   0.030\n",
            "Learning rate for epoch 6 is   0.030\n",
            "Learning rate for epoch 7 is   0.020\n",
            "Learning rate for epoch 8 is   0.020\n",
            "Learning rate for epoch 9 is   0.020\n",
            "Learning rate for epoch 10 is   0.020\n",
            "Learning rate for epoch 11 is   0.005\n",
            "Learning rate for epoch 12 is   0.005\n",
            "Learning rate for epoch 13 is   0.005\n",
            "Learning rate for epoch 14 is   0.005\n",
            "Learning rate for epoch 15 is   0.007\n",
            "Learning rate for epoch 16 is   0.007\n",
            "Learning rate for epoch 17 is   0.007\n",
            "Learning rate for epoch 18 is   0.007\n",
            "Learning rate for epoch 19 is   0.007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ5-uUH_ndKf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_TK0qainAVl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}